{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKv6hKeB_YL0",
        "outputId": "9f2264db-c15e-4862-8e9a-f53362aac13c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oHxUL_IVXk9",
        "outputId": "a675d7c5-724e-49d6-e247-35f6acda3118"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=e4f44577278faa13e70d5b2e6fb9e654e7bfb5af1e80640dc5badf07c999b02d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 1000000"
      ],
      "metadata": {
        "id": "IYuF56k2_CPp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [RandomData] Pyspark vs Python multiprocess"
      ],
      "metadata": {
        "id": "LIhw3_Q17XfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark"
      ],
      "metadata": {
        "id": "XzLa7KP0_jY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "yAW0K3YT_pPJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_into_partitions(X, y, num_partitions=4):\n",
        "    # Split the data into partitions\n",
        "    data_partitions = []\n",
        "    chunk_size = len(X) // num_partitions\n",
        "\n",
        "    for i in range(num_partitions):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = (i + 1) * chunk_size\n",
        "        X_partition = X[start_idx:end_idx]\n",
        "        y_partition = y[start_idx:end_idx]\n",
        "        data_partitions.append((X_partition, y_partition))\n",
        "\n",
        "    return data_partitions\n",
        "\n",
        "def map_function(data_partition, params):\n",
        "    # Compute gradients on a data partition using current model parameters\n",
        "    X, y = data_partition\n",
        "    gradients = np.dot(X.T, np.dot(X, params.value) - y)\n",
        "    return gradients\n",
        "\n",
        "def reduce_function(intermediate_results, learning_rate):\n",
        "    # Combine gradients and update model parameters\n",
        "    total_gradients = np.sum(intermediate_results, axis=0)\n",
        "    updated_params = learning_rate * total_gradients\n",
        "    return updated_params\n",
        "\n",
        "def main():\n",
        "    # Create a SparkSession\n",
        "    spark = SparkSession.builder.appName(\"UNLV\").getOrCreate()\n",
        "\n",
        "    # Generate sample data for linear regression\n",
        "    X = np.random.rand(sample, 3)  # 100 samples with 3 features\n",
        "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(sample)  # Linear relationship with random noise\n",
        "\n",
        "    # Split the data into partitions\n",
        "    data_partitions = split_data_into_partitions(X, y)\n",
        "\n",
        "    # Broadcast the initial model parameters to all workers\n",
        "    params = spark.sparkContext.broadcast(np.zeros(X.shape[1]))\n",
        "\n",
        "    # Set learning rate and number of iterations\n",
        "    learning_rate = 0.1\n",
        "    num_iterations = 100\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in tqdm(range(num_iterations)):\n",
        "        # Parallelize the data partitions\n",
        "        rdd = spark.sparkContext.parallelize(data_partitions)\n",
        "\n",
        "        # Map step: compute gradients on each data partition in parallel\n",
        "        intermediate_results = rdd.map(lambda x: map_function(x, params)).collect()\n",
        "\n",
        "        # Reduce step: combine gradients and update model parameters\n",
        "        params = spark.sparkContext.broadcast(reduce_function(intermediate_results, learning_rate))\n",
        "    end_time = time.time()\n",
        "    print(f\"Time: {(end_time - start_time) // 60}min {(end_time - start_time) % 60:.3f}sec\\n\")\n",
        "\n",
        "    # # Use the final parameters for prediction\n",
        "    # X_test = np.random.rand(100, 3)  # New test data\n",
        "    # y_pred = np.dot(X_test, params.value)\n",
        "    # print(\"Predictions:\", y_pred)\n",
        "\n",
        "    # Stop the SparkSession\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldtrp9Lg6JoC",
        "outputId": "4c3e6623-8b2c-40a5-a86f-03d12a4b4d78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 3)\n",
            "(1000000,)\n",
            "[0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 1.0min 20.584sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python-multiproess"
      ],
      "metadata": {
        "id": "dAPVJfU5_lgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "IsfMsFSE_rEV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_into_partitions(X, y, num_partitions=4):\n",
        "    # Split the data into partitions\n",
        "    data_partitions = []\n",
        "    chunk_size = len(X) // num_partitions\n",
        "\n",
        "    for i in range(num_partitions):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = (i + 1) * chunk_size\n",
        "        X_partition = X[start_idx:end_idx]\n",
        "        y_partition = y[start_idx:end_idx]\n",
        "        data_partitions.append((X_partition, y_partition))\n",
        "\n",
        "    return data_partitions\n",
        "\n",
        "def map_function(data_partition, params):\n",
        "    # Compute gradients on a data partition using current model parameters\n",
        "    X, y = data_partition\n",
        "    gradients = np.dot(X.T, np.dot(X, params) - y)\n",
        "    return gradients\n",
        "\n",
        "def reduce_function(intermediate_results, learning_rate):\n",
        "    # Combine gradients and update model parameters\n",
        "    total_gradients = np.sum(intermediate_results, axis=0)\n",
        "    updated_params = learning_rate * total_gradients\n",
        "    return updated_params\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Generate sample data for linear regression\n",
        "    X = np.random.rand(sample, 3)\n",
        "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(sample)  # Linear relationship with random noise\n",
        "\n",
        "    # Split the data into partitions\n",
        "    data_partitions = split_data_into_partitions(X, y)\n",
        "\n",
        "    # Set initial model parameters\n",
        "    params = np.zeros(X.shape[1])\n",
        "\n",
        "    # Set learning rate and number of iterations\n",
        "    learning_rate = 0.1\n",
        "    num_iterations = 100\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in tqdm(range(num_iterations)):\n",
        "        # Create a multiprocessing Pool\n",
        "        pool = multiprocessing.Pool()\n",
        "\n",
        "        # Map step: compute gradients on each data partition in parallel\n",
        "        intermediate_results = pool.starmap(map_function, [(data_partition, params) for data_partition in data_partitions])\n",
        "\n",
        "        # Reduce step: combine gradients and update model parameters\n",
        "        params = reduce_function(intermediate_results, learning_rate)\n",
        "\n",
        "        # Close the multiprocessing Pool\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTime: {(end_time - start_time) // 60}min {(end_time - start_time) % 60:.3f}sec\\n\")\n",
        "\n",
        "    # # Use the final parameters for prediction\n",
        "    # X_test = np.random.rand(100, 3)  # New test data\n",
        "    # y_pred = np.dot(X_test, params)\n",
        "    # print(\"Predictionxs:\", y_pred)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWMJwX5C7aWZ",
        "outputId": "53a36300-16f1-42d8-bad8-28b2903aab47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:17<00:00,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time: 0.0min 17.678sec\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
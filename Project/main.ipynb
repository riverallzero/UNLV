{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "3dzSMMwx8Y3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import time\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/UNLV/train.csv\")\n",
        "    df.drop([\"Id\"], axis=1, inplace=True)\n",
        "\n",
        "    df.columns = map(str.lower, df.columns)\n",
        "    df.rename(columns={\"married/single\": \"married_single\"}, inplace=True)\n",
        "\n",
        "    # Category cols to num\n",
        "    cate_cols = [\"married_single\", \"profession\", \"house_ownership\", \"car_ownership\", \"city\", \"state\"]\n",
        "\n",
        "    for col in cate_cols:\n",
        "        le = LabelEncoder()\n",
        "        le = le.fit(df[col])\n",
        "        df[col] = le.transform(df[col])\n",
        "\n",
        "    print(\"Label Encoding-Done.\")\n",
        "\n",
        "    # Down sampling\n",
        "    subset_0 = df[df[\"risk_flag\"] == 0]\n",
        "    subset_1 = df[df[\"risk_flag\"] == 1]\n",
        "\n",
        "    subset_0_downsampled = resample(subset_0,\n",
        "                                    replace=False,\n",
        "                                    n_samples=len(subset_1),\n",
        "                                    random_state=42)\n",
        "\n",
        "    df = pd.concat([subset_0_downsampled, subset_1])\n",
        "\n",
        "    print(\"Down Sampling-Done.\")\n",
        "\n",
        "    X = df.drop([\"risk_flag\"], axis=1)\n",
        "    y = df[\"risk_flag\"].apply(lambda x: int(x))\n",
        "\n",
        "    # StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    print(\"Scaling-Done.\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = svm.SVC() # max_iter: default -1\n",
        "\n",
        "    # Start: training\n",
        "    start_time = time.time()\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # End: training\n",
        "    end_time = time.time()\n",
        "    # Calculate training time\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    y_pred = clf.predict(X_val)\n",
        "\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "    print(f\"AUC = {auc:.3f}\")\n",
        "    print(f\"Elapsed Time: {elapsed_time // 60} min {elapsed_time % 60:.2f} sec\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAecy_CQ8bOx",
        "outputId": "f79817f0-8661-41ab-e335-89400335c6b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoding-Done.\n",
            "Down Sampling-Done.\n",
            "Scaling-Done.\n",
            "AUC = 0.641\n",
            "Elapsed Time: 2.0 min 48.30 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM with Spark"
      ],
      "metadata": {
        "id": "murZxQLw8kNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "kmdu4Y9mt1fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3350ef2a-627d-466b-e569-90e0cccf757f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=0fbe34031425abcb4737e9c453aebefa3a4fc0b28430d594edb334de3df0aa2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col as scol\n",
        "from pyspark.sql.functions import rand\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder.appName(\"UNLV\").getOrCreate()\n",
        "\n",
        "    df = spark.read.csv(\"/content/drive/MyDrive/UNLV/train.csv\", header=True, inferSchema=True)\n",
        "    df = df.drop(\"Id\").withColumnRenamed(\"married/single\", \"married_single\")\n",
        "    df = df.toDF(*(col.lower() for col in df.columns))\n",
        "    df = df.withColumn(\"risk_flag\", scol(\"risk_flag\").cast(\"integer\"))\n",
        "\n",
        "    # Category cols to num\n",
        "    cate_cols = [\"married_single\", \"profession\", \"house_ownership\", \"car_ownership\", \"city\", \"state\"]\n",
        "\n",
        "    indexers = [StringIndexer(inputCol=col, outputCol=col + \"_idx\").fit(df) for col in cate_cols]\n",
        "\n",
        "    pipeline = Pipeline(stages=indexers)\n",
        "    df = pipeline.fit(df).transform(df)\n",
        "    df = df.drop(*cate_cols)\n",
        "\n",
        "    print(\"Label Encoding-Done.\")\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[col for col in df.columns if col != \"risk_flag\"],\n",
        "        outputCol=\"origin_features\"\n",
        "    )\n",
        "    df = assembler.transform(df)\n",
        "    df = df.select([\"origin_features\", \"risk_flag\"])\n",
        "\n",
        "    # Down sampling\n",
        "    pos_count = df.filter(\"risk_flag = 1\").count()\n",
        "    neg_df = df.filter(\"risk_flag = 0\")\n",
        "    sampled_neg_df = neg_df.orderBy(rand(seed=42)).limit(pos_count)\n",
        "    df = sampled_neg_df.union(df.filter(\"risk_flag = 1\"))\n",
        "\n",
        "    print(\"Down Sampling-Done.\")\n",
        "\n",
        "    # StandardScaler\n",
        "    scaler = StandardScaler(inputCol=\"origin_features\", outputCol=\"features\")\n",
        "    scaler_model = scaler.fit(df)\n",
        "    df_scaled = scaler_model.transform(df)\n",
        "\n",
        "    df = df_scaled.select([\"features\", \"risk_flag\"])\n",
        "\n",
        "    print(\"Scaling-Done.\")\n",
        "\n",
        "    # Define model\n",
        "    train, val = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    svm = LinearSVC(labelCol=\"risk_flag\", weightCol=\"risk_flag\") # maxIter: default 100\n",
        "\n",
        "    # Start: training\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = svm.fit(train)\n",
        "\n",
        "    # End: training\n",
        "    end_time = time.time()\n",
        "    # Calculate training time\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    predictions = model.transform(val)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"risk_flag\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    print(f\"AUC = {auc:.3f}\")\n",
        "    print(f\"Elapsed Time: {elapsed_time // 60} min {elapsed_time % 60:.2f} sec\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOmC6gIhyd6_",
        "outputId": "6801a1b3-1292-4546-82b8-7c7a22ef6fa9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoding-Done.\n",
            "Down Sampling-Done.\n",
            "Scaling-Done.\n",
            "AUC = 0.503\n",
            "Elapsed Time: 0.0 min 15.18 sec\n"
          ]
        }
      ]
    }
  ]
}